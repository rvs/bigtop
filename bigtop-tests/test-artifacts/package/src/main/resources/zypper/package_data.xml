<?xml version="1.0" encoding="UTF-8"?>
<!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->
<packages>
<mahout>
  <metadata>
    <summary>A set of Java libraries for scalable machine learning.</summary>
    <description>Mahout's goal is to build scalable machine learning libraries.
With scalable we mean:

Scalable to reasonably large data sets. Our core algorithms for clustering,
classfication and batch based collaborative filtering are implemented on top of
Apache Hadoop using the map/reduce paradigm. However we do not restrict
contributions to Hadoop based implementations: Contributions that run on a
single node or on a non-Hadoop cluster are welcome as well. The core libraries
are highly optimized to allow for good performance also for non-distributed
algorithms.
Scalable to support your business case. Mahout is distributed under a
commercially friendly Apache Software license.
Scalable community. The goal of Mahout is to build a vibrant, responsive,
diverse community to facilitate discussions not only on the project itself but
also on potential use cases. Come to the mailing lists to find out more.</description>
    <url>http://mahout.apache.org</url>
  </metadata>
  <deps>
    <hadoop>>=0.20.2</hadoop>
    <bigtop-utils/>
    <tag name="/bin/sh"/>
    <tag name="/bin/bash"/>
  </deps>
  <alternatives>
    <mahout-conf>
      <status>auto</status>
      <link>/etc/mahout/conf</link>
      <value>/etc/mahout/conf.dist</value>
      <alt>/etc/mahout/conf.dist</alt>
    </mahout-conf>
  </alternatives>
</mahout>
<whirr>
  <metadata>
    <summary>Scripts and libraries for running software services on cloud infrastructure.</summary>
    <description>Whirr provides

* A cloud-neutral way to run services. You don't have to worry about the
  idiosyncrasies of each provider.
* A common service API. The details of provisioning are particular to the
  service.
* Smart defaults for services. You can get a properly configured system
  running quickly, while still being able to override settings as needed.</description>
    <url>http://whirr.apache.org/</url>
  </metadata>
  <deps>
    <bigtop-utils/>
    <tag name="/bin/sh"/>
  </deps>
</whirr>
<flume>
  <metadata>
    <summary>Flume is a reliable, scalable, and manageable distributed log collection application for collecting data such as logs and delivering it to data stores such as Hadoop's HDFS.</summary>
    <description>Flume is a reliable, scalable, and manageable distributed data collection
 application for collecting data such as logs and delivering it to data stores
 such as Hadoop's HDFS.  It can efficiently collect, aggregate, and move large
 amounts of log data.  It has a simple, but flexible, architecture based on
 streaming data flows.  It is robust and fault tolerant with tunable reliability
 mechanisms and many failover and recovery mechanisms.  The system is centrally
 managed and allows for intelligent dynamic management.  It uses a simple
 extensible data model that allows for online analytic applications.</description>
    <url>http://incubator.apache.org/projects/flume.html</url>
  </metadata>
  <deps>
    <coreutils/>
    <tag name="/usr/sbin/useradd"/>
    <hadoop/>
    <bigtop-utils/>
    <sh-utils/>
    <tag name="/bin/sh"/>
    <tag name="/bin/bash"/>
  </deps>
  <groups>
    <flume>
      <user>flume</user>
    </flume>
  </groups>
  <users>
    <flume>
      <home>/var/run/flume</home>
      <descr>Flume</descr>
      <shell>/sbin/nologin</shell>
    </flume>
  </users>
  <alternatives>
    <flume-conf>
      <status>auto</status>
      <link>/etc/flume/conf</link>
      <value>/etc/flume/conf.empty</value>
      <alt>/etc/flume/conf.empty</alt>
    </flume-conf>
  </alternatives>
</flume>
<flume-node>
  <metadata>
    <summary>The flume node daemon is a core element of flume's data path and is responsible for generating, processing, and delivering data.</summary>
    <description>Flume is a reliable, scalable, and manageable distributed data collection application for collecting data such as logs and delivering it to data stores such as Hadoop's HDFS. It can efficiently collect, aggregate, and move large amounts of log data. It has a simple, but flexible, architecture based on streaming data flows. It is robust and fault tolerant with tunable reliability mechanisms and many failover and recovery mechanisms. The system is centrally managed and allows for intelligent dynamic management. It uses a simple extensible data model that allows for online analytic applications.</description>
    <url>http://incubator.apache.org/projects/flume.html</url>
  </metadata>
  <deps>
    <flume>/self</flume>
    <tag name="/sbin/service"/>
    <tag name="/sbin/chkconfig"/>
    <insserv/>
    <tag name="/bin/sh"/>
    <tag name="/bin/bash"/>
  </deps>
  <services>
    <flume-node>
       <!-- BUG https://jira.cloudera.com/browse/KITCHEN-1095 --><runlevel>3</runlevel><runlevel>4</runlevel><runlevel>5</runlevel>
       <oninstall>stop</oninstall>
       <configured>true</configured>
    </flume-node>
  </services>
</flume-node>
<sqoop>
  <metadata>
    <summary>Sqoop allows easy imports and exports of data sets between databases and the Hadoop Distributed File System (HDFS).</summary>
    <description>Sqoop allows easy imports and exports of data sets between databases and the Hadoop Distributed File System (HDFS).</description>
    <url>http://incubator.apache.org/sqoop/</url>
  </metadata>
  <deps>
    <hadoop/>
    <bigtop-utils/>
    <tag name="/bin/sh"/>
    <tag name="/bin/bash"/>
  </deps>
  <alternatives>
    <sqoop-conf>
      <status>auto</status>
      <link>/etc/sqoop/conf</link>
      <value>/etc/sqoop/conf.dist</value>
      <alt>/etc/sqoop/conf.dist</alt>
    </sqoop-conf>
  </alternatives>
</sqoop>
<sqoop-metastore>
  <metadata>
    <summary>Shared metadata repository for Sqoop.</summary>
    <description>Shared metadata repository for Sqoop. This optional package hosts a metadata server for Sqoop clients across a network to use.</description>
    <url>http://incubator.apache.org/sqoop/</url>
  </metadata>
  <deps>
    <sqoop>/self</sqoop>
    <insserv/>
    <tag name="/bin/bash"/>
    <tag name="/bin/sh"/>
  </deps>
  <services>
    <sqoop-metastore>
       <!-- BUG https://jira.cloudera.com/browse/KITCHEN-1095 --><runlevel>3</runlevel><runlevel>4</runlevel><runlevel>5</runlevel>
       <oninstall>stop</oninstall>
       <configured>true</configured>
    </sqoop-metastore>
  </services>
  <groups>
    <sqoop>
      <user>sqoop</user>
    </sqoop>
  </groups>
  <users>
    <sqoop>
      <home>/var/lib/sqoop</home>
      <descr>Sqoop</descr>
      <shell>/sbin/nologin</shell>
    </sqoop>
  </users>
</sqoop-metastore>
<oozie>
  <metadata>
    <summary>Oozie is a system that runs workflows of Hadoop jobs.</summary>
    <description>Oozie is a system that runs workflows of Hadoop jobs.
 Oozie workflows are actions arranged in a control dependency DAG (Direct
 Acyclic Graph).

 Oozie coordinator functionality allows to start workflows at regular
 frequencies and when data becomes available in HDFS.

 An Oozie workflow may contain the following types of actions nodes:
 map-reduce, map-reduce streaming, map-reduce pipes, pig, file-system,
 sub-workflows, java, hive, sqoop and ssh (deprecated).

 Flow control operations within the workflow can be done using decision,
 fork and join nodes. Cycles in workflows are not supported.

 Actions and decisions can be parameterized with job properties, actions
 output (i.e. Hadoop counters) and HDFS  file information (file exists,
 file size, etc). Formal parameters are expressed in the workflow definition
 as ${VAR} variables.

 A Workflow application is an HDFS directory that contains the workflow
 definition (an XML file), all the necessary files to run all the actions:
 JAR files for Map/Reduce jobs, shells for streaming Map/Reduce jobs, native
 libraries, Pig scripts, and other resource files.

 Running workflow jobs is done via command line tools, a WebServices API
 or a Java API.

 Monitoring the system and workflow jobs can be done via a web console, the
 command line tools, the WebServices API and the Java API.

 Oozie is a transactional system and it has built in automatic and manual
 retry capabilities.

 In case of workflow job failure, the workflow job can be rerun skipping
 previously completed actions, the workflow application can be patched before
 being rerun.</description>
    <url>http://incubator.apache.org/oozie/</url>
  </metadata>
  <deps>
    <tag name="/usr/sbin/groupadd"/>
    <tag name="/usr/sbin/useradd"/>
    <tag name="/sbin/chkconfig"/>
    <hadoop/>
    <tag name="/sbin/service"/>
    <zip/>
    <unzip/>
    <oozie-client>/self</oozie-client>
    <tag name="/bin/sh"/>
    <tag name="/bin/bash"/>
  </deps>
  <services>
    <oozie>
       <!-- BUG https://jira.cloudera.com/browse/KITCHEN-1095 --><runlevel>2</runlevel><runlevel>3</runlevel><runlevel>4</runlevel><runlevel>5</runlevel>
       <oninstall>stop</oninstall>
       <configured>true</configured>
    </oozie>
  </services>
  <groups>
    <oozie>
      <user>oozie</user>
    </oozie>
  </groups>
  <users>
    <oozie>
      <home>/var/run/oozie</home>
      <descr>Oozie User</descr>
      <shell>/bin/false</shell>
    </oozie>
  </users>
</oozie>
<oozie-client>
  <metadata>
    <summary>Client for Oozie Workflow Engine</summary>
    <description>Oozie client is a command line client utility that allows remote
 administration and monitoring of worflows. Using this client utility
 you can submit worflows, start/suspend/resume/kill workflows and
 find out their status at any instance. Apart from such operations,
 you can also change the status of the entire system, get vesion
 information. This client utility also allows you to validate
 any worflows before they are deployed to the Oozie server.</description>
    <url>http://incubator.apache.org/oozie/</url>
  </metadata>
  <deps>
    <bigtop-utils/>
    <tag name="/bin/bash"/>
    <tag name="/bin/sh"/>
  </deps>
  <alternatives>
    <oozie-conf>
      <status>auto</status>
      <link>/etc/oozie/conf</link>
      <value>/etc/oozie/conf.dist</value>
      <alt>/etc/oozie/conf.dist</alt>
    </oozie-conf>
  </alternatives>
</oozie-client>
<zookeeper>
  <metadata>
    <summary>A high-performance coordination service for distributed applications.</summary>
    <description>ZooKeeper is a centralized service for maintaining configuration information,
naming, providing distributed synchronization, and providing group services.
All of these kinds of services are used in some form or another by distributed
applications. Each time they are implemented there is a lot of work that goes
into fixing the bugs and race conditions that are inevitable. Because of the
difficulty of implementing these kinds of services, applications initially
usually skimp on them ,which make them brittle in the presence of change and
difficult to manage. Even when done correctly, different implementations of these services lead to management complexity when the applications are deployed.</description>
    <url>http://zookeeper.apache.org/</url>
    <!-- group>misc</group -->
  </metadata>
  <deps>
    <coreutils/>
    <shadow-utils/>
    <tag name="/usr/sbin/groupadd"/>
    <tag name="/usr/sbin/useradd"/>
    <update-alternatives/>
    <bigtop-utils/>
    <tag name="/bin/sh"/>
    <tag name="/usr/bin/env"/>
  </deps>
  <groups>
    <zookeeper>
      <user>zookeeper</user>
    </zookeeper>
  </groups>
  <users>
    <zookeeper>
      <home>/var/run/zookeeper</home>
      <descr>ZooKeeper</descr>
      <shell>/sbin/nologin</shell>
    </zookeeper>
  </users>
  <alternatives>
    <zookeeper-conf>
      <status>auto</status>
      <link>/etc/zookeeper/conf</link>
      <value>/etc/zookeeper/conf.dist</value>
      <alt>/etc/zookeeper/conf.dist</alt>
    </zookeeper-conf>
  </alternatives>
</zookeeper>
<zookeeper-server>
  <metadata>
    <!-- license>APL2</license -->
    <!-- arch>universal</arch -->
    <summary>The Hadoop Zookeeper server</summary>
    <description>This package starts the zookeeper server on startup</description>
    <url>http://zookeeper.apache.org/</url>
    <!-- vendor>(none)</vendor -->
    <!-- group>Development/Libraries</group -->
    <!-- depends><dep>adduser</dep><dep>sun-java6-jre</dep><dep>sun-java6-bin</dep></depends -->
    <!-- breaks></breaks -->
    <!-- replaces></replaces -->
    <!-- provides>zookeeper</provides -->
  </metadata>
  <deps>
    <zookeeper>/self</zookeeper>
    <aaa_base/>
    <insserv/>
    <tag name="/bin/sh"/>
    <tag name="/bin/bash"/>
  </deps>
  <services>
    <zookeeper-server>
      <!-- BUG https://jira.cloudera.com/browse/KITCHEN-1095 --><runlevel>3</runlevel><runlevel>4</runlevel><runlevel>5</runlevel>
      <oninstall>stop</oninstall>
      <configured>true</configured>
    </zookeeper-server>
  </services>
</zookeeper-server>
<pig>
  <metadata>
    <summary>Pig is a platform for analyzing large data sets</summary>
    <description>Pig is a platform for analyzing large data sets that consists of a high-level language
 for expressing data analysis programs, coupled with infrastructure for evaluating these
 programs. The salient property of Pig programs is that their structure is amenable
 to substantial parallelization, which in turns enables them to handle very large data sets.

 At the present time, Pig's infrastructure layer consists of a compiler that produces
 sequences of Map-Reduce programs, for which large-scale parallel implementations already
 exist (e.g., the Hadoop subproject). Pig's language layer currently consists of a textual
 language called Pig Latin, which has the following key properties:
 
 * Ease of programming
   It is trivial to achieve parallel execution of simple, "embarrassingly parallel" data
   analysis tasks. Complex tasks comprised of multiple interrelated data transformations
   are explicitly encoded as data flow sequences, making them easy to write, understand,
   and maintain.
 * Optimization opportunities
   The way in which tasks are encoded permits the system to optimize their execution
   automatically, allowing the user to focus on semantics rather than efficiency.
 * Extensibility
   Users can create their own functions to do special-purpose processing.</description>
    <url>http://pig.apache.org/</url>
  </metadata>
  <deps>
    <hadoop/>
    <bigtop-utils/>
    <tag name="/bin/sh"/>
    <tag name="/usr/bin/env"/>
  </deps>
  <alternatives>
    <pig-conf>
      <status>auto</status>
      <link>/etc/pig/conf</link>
      <value>/etc/pig/conf.dist</value>
      <alt>/etc/pig/conf.dist</alt>
    </pig-conf>
  </alternatives>
</pig>
<hive>
  <metadata>
    <summary>Hive is a data warehouse infrastructure built on top of Hadoop</summary>
    <description>Hive is a data warehouse infrastructure built on top of Hadoop that
 provides tools to enable easy data summarization, adhoc querying and
 analysis of large datasets data stored in Hadoop files. It provides a
 mechanism to put structure on this data and it also provides a simple
 query language called Hive QL which is based on SQL and which enables
 users familiar with SQL to query this data. At the same time, this
 language also allows traditional map/reduce programmers to be able to
 plug in their custom mappers and reducers to do more sophisticated
 analysis which may not be supported by the built-in capabilities of
 the language.</description>
    <url>http://hive.apache.org/</url>
  </metadata>
  <deps>
    <hadoop>>=0.20.2</hadoop>
    <bigtop-utils/>
    <tag name="/bin/sh"/>
    <tag name="/usr/bin/env"/>
  </deps>
  <alternatives>
    <hive-conf>
      <status>auto</status>
      <value>/etc/hive/conf.dist</value>
      <link>/etc/hive/conf</link>
      <alt>/etc/hive/conf.dist</alt>
    </hive-conf>
  </alternatives>
</hive>
<hbase>
  <metadata>
    <summary>HBase is the Hadoop database. Use it when you need random, realtime read/write access to your Big Data. This project's goal is the hosting of very large tables -- billions of rows X millions of columns -- atop clusters of commodity hardware.</summary>
    <description>HBase is an open-source, distributed, column-oriented store modeled after Google' Bigtable: A Distributed Storage System for Structured Data by Chang et al. Just as Bigtable leverages the distributed data storage provided by the Google File System, HBase provides Bigtable-like capabilities on top of Hadoop. HBase includes:

    * Convenient base classes for backing Hadoop MapReduce jobs with HBase tables
    * Query predicate push down via server side scan and get filters
    * Optimizations for real time queries
    * A high performance Thrift gateway
    * A REST-ful Web service gateway that supports XML, Protobuf, and binary data encoding options
    * Cascading source and sink modules
    * Extensible jruby-based (JIRB) shell
    * Support for exporting metrics via the Hadoop metrics subsystem to files or Ganglia; or via JMX</description>
    <url>http://hbase.apache.org/</url>
    <!-- group>misc</group -->
  </metadata>
  <deps>
    <coreutils/>
    <tag name="/usr/sbin/useradd"/>
    <tag name="/sbin/chkconfig"/>
    <tag name="/sbin/service"/>
    <hadoop>>=0.20.2</hadoop>
    <zookeeper>>=3.3.1</zookeeper>
    <bigtop-utils/>
    <sh-utils/>
    <tag name="/bin/sh"/>
    <tag name="/usr/bin/env"/>
  </deps>
  <alternatives>
    <hbase-conf>
      <status>auto</status>
      <value>/etc/hbase/conf.dist</value>
      <link>/etc/hbase/conf</link>
      <alt>/etc/hbase/conf.dist</alt>
    </hbase-conf>
  </alternatives>
  <groups>
    <hbase>
      <user>hbase</user>
    </hbase>
  </groups>
  <users>
    <hbase>
      <home>/var/run/hbase</home> <!-- BUG https://issues.cloudera.org/browse/DISTRO-231 -->
      <descr>HBase</descr>
      <shell>/sbin/nologin</shell>
    </hbase>
  </users>
</hbase>
<hbase-doc>
  <metadata>
    <summary>Hbase Documentation</summary>
    <description>Documentation for Hbase</description>
    <url>http://hbase.apache.org/</url>
    <!-- group>misc</group -->
  </metadata>
</hbase-doc>
<hbase-master>
  <metadata>
    <summary>The Hadoop HBase master Server.</summary>
    <description>HMaster is the "master server" for a HBase. There is only one HMaster for a single HBase deployment.</description>
    <url>http://hbase.apache.org/</url>
  </metadata>
  <deps>
    <hbase>/self</hbase>
    <insserv/>
    <tag name="/bin/sh"/>
    <tag name="/bin/bash"/>
  </deps>
  <services>
    <hbase-master>
       <!-- BUG https://jira.cloudera.com/browse/KITCHEN-1095 --><runlevel>3</runlevel><runlevel>4</runlevel><runlevel>5</runlevel>
       <oninstall>stop</oninstall>
       <configured>true</configured>
    </hbase-master>
  </services>
</hbase-master>
<hbase-regionserver>
  <metadata>
    <summary>The Hadoop HBase RegionServer server.</summary>
    <description>HRegionServer makes a set of HRegions available to clients. It checks in with the HMaster. There are many HRegionServers in a single HBase deployment.</description>
    <url>http://hbase.apache.org/</url>
  </metadata>
  <deps>
    <hbase>/self</hbase>
    <insserv/>
    <tag name="/bin/sh"/>
    <tag name="/bin/bash"/>
  </deps>
  <services>
    <hbase-regionserver>
       <!-- BUG https://jira.cloudera.com/browse/KITCHEN-1095 --><runlevel>3</runlevel><runlevel>4</runlevel><runlevel>5</runlevel>
       <oninstall>stop</oninstall>
       <configured>false</configured>
    </hbase-regionserver>
  </services>
</hbase-regionserver>
<hbase-thrift>
  <metadata>
    <summary>The Hadoop HBase Thrift Interface</summary>
    <description>ThriftServer - this class starts up a Thrift server which implements the Hbase API specified in the Hbase.thrift IDL file. "Thrift is a software framework for scalable cross-language services development. It combines a powerful software stack with a code generation engine to build services that work efficiently and seamlessly between C++, Java, Python, PHP, and Ruby. Thrift was developed at Facebook, and we are now releasing it as open source." For additional information, see http://developers.facebook.com/thrift/. Facebook has announced their intent to migrate Thrift into Apache Incubator.</description>
    <url>http://hbase.apache.org/</url>
  </metadata>
  <deps>
    <hbase>/self</hbase>
    <insserv/>
    <tag name="/bin/sh"/>
    <tag name="/bin/bash"/>
  </deps>
  <services>
    <hbase-thrift>
       <!-- BUG https://jira.cloudera.com/browse/KITCHEN-1095 --><runlevel>3</runlevel><runlevel>4</runlevel><runlevel>5</runlevel>
       <oninstall>stop</oninstall>
       <configured>false</configured>
    </hbase-thrift>
  </services>
</hbase-thrift>
<hadoop>
  <metadata>
    <summary>Hadoop is a software platform for processing vast amounts of data</summary>
    <description>Hadoop is a software platform that lets one easily write and
run applications that process vast amounts of data.

Here's what makes Hadoop especially useful:
* Scalable: Hadoop can reliably store and process petabytes.
* Economical: It distributes the data and processing across clusters
              of commonly available computers. These clusters can number
              into the thousands of nodes.
* Efficient: By distributing the data, Hadoop can process it in parallel
             on the nodes where the data is located. This makes it
             extremely rapid.
* Reliable: Hadoop automatically maintains multiple copies of data and
            automatically redeploys computing tasks based on failures.

Hadoop implements MapReduce, using the Hadoop Distributed File System (HDFS).
MapReduce divides applications into many small blocks of work. HDFS creates
multiple replicas of data blocks for reliability, placing them on compute
nodes around the cluster. MapReduce can then process the data where it is
located.</description>
    <url>http://hadoop.apache.org/core/</url>
    <!-- group>misc</group -->
  </metadata>
  <deps>
    <coreutils/>
    <tag name="/usr/sbin/useradd"/>
    <tag name="/usr/sbin/usermod"/>
    <tag name="/sbin/chkconfig"/>
    <tag name="/sbin/service"/>
    <bigtop-utils/> 
    <sh-utils/>
    <insserv/>
    <tag name="/bin/sh"/>
    <tag name="/usr/bin/env"/>
  </deps>
  <groups>
    <hadoop>
      <user>hdfs</user>
      <user>mapred</user>
    </hadoop>
    <hdfs>
      <user>hdfs</user>
    </hdfs>
    <mapred>
      <user>mapred</user>
    </mapred>
  </groups>
  <users>
    <hdfs>
      <home>/usr/lib/hadoop</home>
      <descr>Hadoop HDFS</descr>
      <shell>/bin/bash</shell>
    </hdfs>
    <mapred>
      <home>/usr/lib/hadoop</home>
      <descr>Hadoop MapReduce</descr>
      <shell>/bin/bash</shell>
    </mapred>
  </users>
  <alternatives>
    <hadoop-conf>
      <status>auto</status>
      <link>/etc/hadoop/conf</link>
      <value>/etc/hadoop/conf.empty</value>
      <alt>/etc/hadoop/conf.empty</alt>
    </hadoop-conf>
  </alternatives>
</hadoop>
<hadoop-pipes>
  <metadata>
    <!-- license>APL2</license -->
    <!-- arch>universal</arch -->
    <summary>Hadoop Pipes Library</summary>
    <description>Hadoop Pipes Library</description>
    <url>http://hadoop.apache.org/core/</url>
    <!-- vendor>(none)</vendor -->
    <!-- group>Development/Libraries</group -->
    <!-- depends><dep>adduser</dep><dep>sun-java6-jre</dep><dep>sun-java6-bin</dep></depends -->
    <!-- breaks></breaks -->
    <!-- replaces></replaces -->
    <!-- provides>zookeeper</provides -->
  </metadata>
  <deps>
    <hadoop>/self</hadoop>
  </deps>
</hadoop-pipes>
<hadoop-native>
  <metadata>
    <!-- license>APL2</license -->
    <!-- arch>universal</arch -->
    <summary>Native libraries for Hadoop Compression</summary>
    <description>Native libraries for Hadoop compression</description>
    <url>http://hadoop.apache.org/core/</url>
    <!-- vendor>(none)</vendor -->
    <!-- group>Development/Libraries</group -->
    <!-- depends><dep>adduser</dep><dep>sun-java6-jre</dep><dep>sun-java6-bin</dep></depends -->
    <!-- breaks></breaks -->
    <!-- replaces></replaces -->
    <!-- provides>zookeeper</provides -->
  </metadata>
  <deps>
    <hadoop>/self</hadoop>
  </deps>
</hadoop-native>
<hadoop-namenode>
  <metadata>
    <!-- license>APL2</license -->
    <!-- arch>universal</arch -->
    <summary>The Hadoop namenode manages the block locations of HDFS files</summary>
    <description>The Hadoop Distributed Filesystem (HDFS) requires one unique server, the
namenode, which manages the block locations of files on the filesystem.</description>
    <url>http://hadoop.apache.org/core/</url>
    <!-- vendor>(none)</vendor -->
    <!-- group>Development/Libraries</group -->
    <!-- depends><dep>adduser</dep><dep>sun-java6-jre</dep><dep>sun-java6-bin</dep></depends -->
    <!-- breaks></breaks -->
    <!-- replaces></replaces -->
    <!-- provides>zookeeper</provides -->
  </metadata>
  <deps>
    <hadoop>/self</hadoop>
    <tag name="/bin/sh"/>
    <tag name="/bin/bash"/>
  </deps>
  <services>
    <hadoop-namenode>
      <!-- BUG https://jira.cloudera.com/browse/KITCHEN-1095 --><runlevel>3</runlevel><runlevel>4</runlevel><runlevel>5</runlevel>
      <oninstall>stop</oninstall>
      <configured>false</configured>
    </hadoop-namenode>
  </services>
</hadoop-namenode>
<hadoop-secondarynamenode>
  <metadata>
    <!-- license>APL2</license -->
    <!-- arch>universal</arch -->
    <summary>Hadoop Secondary namenode</summary>
    <description>The Secondary Name Node periodically compacts the Name Node EditLog
into a checkpoint.  This compaction ensures that Name Node restarts
do not incur unnecessary downtime.</description>
    <url>http://hadoop.apache.org/core/</url>
    <!-- vendor>(none)</vendor -->
    <!-- group>Development/Libraries</group -->
    <!-- depends><dep>adduser</dep><dep>sun-java6-jre</dep><dep>sun-java6-bin</dep></depends -->
    <!-- breaks></breaks -->
    <!-- replaces></replaces -->
    <!-- provides>zookeeper</provides -->
  </metadata>
  <deps>
    <hadoop>/self</hadoop>
    <tag name="/bin/sh"/>
    <tag name="/bin/bash"/>
  </deps>
  <services>
    <hadoop-secondarynamenode>
      <!-- BUG https://jira.cloudera.com/browse/KITCHEN-1095 --><runlevel>3</runlevel><runlevel>4</runlevel><runlevel>5</runlevel>
      <oninstall>stop</oninstall>
      <configured>false</configured>
    </hadoop-secondarynamenode>
  </services>
</hadoop-secondarynamenode>
<hadoop-datanode>
  <metadata>
    <!-- license>APL2</license -->
    <!-- arch>universal</arch -->
    <summary>Hadoop Data Node</summary>
    <description>The Data Nodes in the Hadoop Cluster are responsible for serving up
blocks of data over the network to Hadoop Distributed Filesystem
(HDFS) clients.</description>
    <url>http://hadoop.apache.org/core/</url>
    <!-- vendor>(none)</vendor -->
    <!-- group>Development/Libraries</group -->
    <!-- depends><dep>adduser</dep><dep>sun-java6-jre</dep><dep>sun-java6-bin</dep></depends -->
    <!-- breaks></breaks -->
    <!-- replaces></replaces -->
    <!-- provides>zookeeper</provides -->
  </metadata>
  <deps>
    <hadoop>/self</hadoop>
    <tag name="/bin/sh"/>
    <tag name="/bin/bash"/>
  </deps>
  <services>
    <hadoop-datanode>
      <!-- BUG https://jira.cloudera.com/browse/KITCHEN-1095 --><runlevel>3</runlevel><runlevel>4</runlevel><runlevel>5</runlevel>
      <oninstall>stop</oninstall>
      <configured>false</configured>
    </hadoop-datanode>
  </services>
</hadoop-datanode>
<hadoop-jobtracker>
  <metadata>
    <!-- license>APL2</license -->
    <!-- arch>universal</arch -->
    <summary>Hadoop Job Tracker</summary>
    <description>The jobtracker is a central service which is responsible for managing
the tasktracker services running on all nodes in a Hadoop Cluster.
The jobtracker allocates work to the tasktracker nearest to the data
with an available work slot.</description>
    <url>http://hadoop.apache.org/core/</url>
    <!-- vendor>(none)</vendor -->
    <!-- group>Development/Libraries</group -->
    <!-- depends><dep>adduser</dep><dep>sun-java6-jre</dep><dep>sun-java6-bin</dep></depends -->
    <!-- breaks></breaks -->
    <!-- replaces></replaces -->
    <!-- provides>zookeeper</provides -->
  </metadata>
  <deps>
    <hadoop>/self</hadoop>
    <tag name="/bin/sh"/>
    <tag name="/bin/bash"/>
  </deps>
  <services>
    <hadoop-jobtracker>
      <!-- BUG https://jira.cloudera.com/browse/KITCHEN-1095 --><runlevel>3</runlevel><runlevel>4</runlevel><runlevel>5</runlevel>
      <oninstall>stop</oninstall>
      <configured>false</configured>
    </hadoop-jobtracker>
  </services>
</hadoop-jobtracker>
<hadoop-tasktracker>
  <metadata>
    <!-- license>APL2</license -->
    <!-- arch>universal</arch -->
    <summary>Hadoop Task Tracker</summary>
    <description>The tasktracker has a fixed number of work slots.  The jobtracker
assigns MapReduce work to the tasktracker that is nearest the data
with an available work slot.</description>
    <url>http://hadoop.apache.org/core/</url>
    <!-- vendor>(none)</vendor -->
    <!-- group>Development/Libraries</group -->
    <!-- depends><dep>adduser</dep><dep>sun-java6-jre</dep><dep>sun-java6-bin</dep></depends -->
    <!-- breaks></breaks -->
    <!-- replaces></replaces -->
    <!-- provides>zookeeper</provides -->
  </metadata>
  <deps>
    <hadoop>/self</hadoop>
    <tag name="/bin/sh"/>
    <tag name="/bin/bash"/>
  </deps>
  <services>
    <hadoop-tasktracker>
      <!-- BUG https://jira.cloudera.com/browse/KITCHEN-1095 --><runlevel>3</runlevel><runlevel>4</runlevel><runlevel>5</runlevel>
      <oninstall>stop</oninstall>
      <configured>false</configured>
    </hadoop-tasktracker>
  </services>
</hadoop-tasktracker>
<hadoop-conf-pseudo>
  <metadata>
    <!-- license>APL2</license -->
    <!-- arch>universal</arch -->
    <summary>Hadoop installation in pseudo-distributed mode</summary>
    <description>Installation of this RPM will setup your machine to run in pseudo-distributed mode
where each Hadoop daemon runs in a separate Java process.</description>
    <url>http://hadoop.apache.org/core/</url>
    <!-- vendor>(none)</vendor -->
    <!-- group>Development/Libraries</group -->
    <!-- depends><dep>adduser</dep><dep>sun-java6-jre</dep><dep>sun-java6-bin</dep></depends -->
    <!-- breaks></breaks -->
    <!-- replaces></replaces -->
    <!-- provides>zookeeper</provides -->
  </metadata>
  <deps>
    <hadoop>/self</hadoop>
    <hadoop-namenode>/self</hadoop-namenode>
    <hadoop-secondarynamenode>/self</hadoop-secondarynamenode>
    <hadoop-datanode>/self</hadoop-datanode>
    <hadoop-jobtracker>/self</hadoop-jobtracker>
    <hadoop-tasktracker>/self</hadoop-tasktracker>
    <tag name="/bin/sh"/>
  </deps>
</hadoop-conf-pseudo>
<hadoop-doc>
  <metadata>
    <!-- license>APL2</license -->
    <!-- arch>universal</arch -->
    <summary>Hadoop Documentation</summary>
    <description>Documentation for Hadoop</description>
    <url>http://hadoop.apache.org/core/</url>
    <!-- vendor>(none)</vendor -->
    <!-- group>Development/Libraries</group -->
    <!-- depends><dep>adduser</dep><dep>sun-java6-jre</dep><dep>sun-java6-bin</dep></depends -->
    <!-- breaks></breaks -->
    <!-- replaces></replaces -->
    <!-- provides>zookeeper</provides -->
  </metadata>
</hadoop-doc>
<hadoop-source>
  <metadata>
    <!-- license>APL2</license -->
    <!-- arch>universal</arch -->
    <summary>Source code for Hadoop</summary>
    <description>The Java source code for Hadoop and its contributed packages. This is handy when
trying to debug programs that depend on Hadoop.</description>
    <url>http://hadoop.apache.org/core/</url>
    <!-- vendor>(none)</vendor -->
    <!-- group>Development/Libraries</group -->
    <!-- depends><dep>adduser</dep><dep>sun-java6-jre</dep><dep>sun-java6-bin</dep></depends -->
    <!-- breaks></breaks -->
    <!-- replaces></replaces -->
    <!-- provides>zookeeper</provides -->
  </metadata>
</hadoop-source>
<hadoop-libhdfs>
  <metadata>
    <!-- license>APL2</license -->
    <!-- arch>universal</arch -->
    <summary>Hadoop Filesystem Library</summary>
    <description>Hadoop Filesystem Library</description>
    <url>http://hadoop.apache.org/core/</url>
    <!-- vendor>(none)</vendor -->
    <!-- group>Development/Libraries</group -->
    <!-- depends><dep>adduser</dep><dep>sun-java6-jre</dep><dep>sun-java6-bin</dep></depends -->
    <!-- breaks></breaks -->
    <!-- replaces></replaces -->
    <!-- provides>zookeeper</provides -->
  </metadata>
  <deps>
    <hadoop>/self</hadoop>
  </deps>
</hadoop-libhdfs>

<!-- hadoop-debuginfo>
  <metadata>
    <summary>Debug information for package hadoop</summary>
    <description>This package provides debug information for package hadoop.
Debug information is useful when developing applications that use this
package or when debugging this package.</description>
    <url>http://hadoop.apache.org/core/</url>
  </metadata>
</hadoop-debuginfo -->
</packages>
